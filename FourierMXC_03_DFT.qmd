# The discrete Fourier transform

```{python}
import numpy as np
import math
import matplotlib.pyplot as plt
import scipy.fftpack
import random
from mpl_toolkits.mplot3d import Axes3D
```

## How it works

### Pseudocode for the DFT

Create a loop going through each point in the signal. For each iteration $i$,
create a complex sine wave with the same length as the signal, and a frequency
defined as $i-1$. Then, calculate the dot product between the wave and the
signal. The result is called a **Fourier coefficient**, from which it is
possible to extract the amplitude of the angle relative to the positive real
axis, as well as the magnitude. The latter represents the signal amplitude for
each frequency, which can be squared to obtain their powers.

### Python

The first step is to simulate a signal as the sum two sine waves, one at $4$
hertz, with an amplitude of $2.5$ and the another other at $6.5$ hertz, with
an amplitude of $1.5$ and this sine wave has an amplitude of two point five.
Simulating data is useful for learning the methodologies because you know the
signal that was generated, making the outcome easier to predict.

```{python}
# create the signal
srate  = 1000 # hz
time   = np.arange(0.,2.,1/srate) # time vector in seconds
pnts   = len(time) # number of time points
signal = 2.5 * np.sin( 2*np.pi*4*time ) + 1.5 * np.sin( 2*np.pi*6.5*time )
```

Then we create a vector holding the complex sine waves ranging from 0 to near-1.
Notice the count only reaches the length of the signal minus on before dividing
by the length of the signal (number of time points). This may seem like an odd
detail, but it is important to ensure the time vector is normalized like this
[SEE VIDEO ABOUT NORMALIZATION]. We also prepare an output vector with the same
length as the signal, where we will place the fourier coefficients.

```{python}
# prepare the Fourier transform
fourTime = np.array(range(pnts))/pnts
fCoefs   = np.zeros((len(signal)),dtype=complex)
```

For each iteration within the loop we take the $i^{th}$ frequency in `NAME` and
use it to create a complex sine wave (`csw`). Once we have the signal we can
calculate the dot product between the original sinal and the complex wave we
just have generated.

This dot product is the reason why the normalized time needs to have the same
number of points as the original signal: otherwise we would not be able to
compute a valid product between the sine wave and the signal.

```{python}
for fi in range(pnts):
    
    # create complex sine wave
    csw = np.exp( -1j*2*np.pi*fi*fourTime )
    
    # compute dot product between sine wave and signal
    # these are called the Fourier coefficients
    fCoefs[fi] = np.sum( np.multiply(signal,csw) ) / pnts
```

~~Note this means the number of frequencies of the Fourier transform is controlled by the number of points in the signal!. There are several important aspects to the Fourier transform that you need to know about, but that will have to wait because there are other aspects that you need to learn first.~~

After completing the loop we have a fourier coefficient per frequency. Now we
can extract the magnitude, the distance of each coefficient away from the $0$.
To obtain the signal amplitude in the units of the original signal requires
multiplying the magnitude by two [SEE OTHER VIDEO].

Now we plot the amplitude spectrum from our Fourier transform here. The time
domain looks like [MAKE PLOT], whereas the amplitude in the frequency domain
looks like:

```{python}
# extract amplitudes
ampls = 2*np.abs(fCoefs)

# compute frequencies vector
hz = np.linspace(0,srate/2,int(math.floor(pnts/2.)+1))

plt.stem(hz,ampls[range(len(hz))])
plt.xlabel('Frequency (Hz)'), plt.ylabel('Amplitude (a.u.)')
plt.xlim(0,10)
plt.show()

```

Most of the Fourier coefficients are zero, except for $4$ and $6.5$, the two
frequencies of the waves we added to generate the signal.

An important detail about this visualization is that using a line-plot to show
the results of a Fourier transform is not fully correct because the drawing a
line implies we know the amplitude between points. However, it is not possible
to measure what the signal looks like with an infinite level of detail, we only
know is value at specific frequencies. Thus, it is more correct to use a stem
plot or a bar plot to convey the results of a Fourier transform.

We can also compare these results with the output of the fast Fourier transform
FFT function: we apply the function, and extract the amplitudes from the
Fourier coefficients. As we can see the result of the function and the
loop-based result overlap. This is not a rigorous proof that these procedures
are accurate, but the convergent results do provide evidence that we're doing
the right thing here, because I think we can all agree to trust the Numpy FFT
code.

### Second plot

The last plot was just a way to represent information contained in the Fourier
coefficients. Now I'm going to plot two Fourier coefficients at $4$ and $4.5$
hertz. For this, I need to find the two indices in the frequency vector that
correspond to these two frequencies [SEE NEXT VIDEO]. [...] For now, you can
just assume that this is a vector that converts from frequencies in indices to
frequencies in Hertz, given our sampling rate and given the length of time of
the signal. Now I extract the magnitude and the angle of the coefficients and
plot them on the complex plane (real axis, $x$, imaginary axis $y$). Larger 
distances from the origin (magnitude) indicate the signal is more similar to
a sine wave at that specific frequency. Thus, $4.5$, a frequency appearing in
the signal is away from the origin.

If the dot product is really close to the origin, then it means that the signal
and the sine wave at this frequency are close to orthogonal, which means that
the signal looks nothing like the sine wave at that frequency. So $4$ is very
close to $0$

When doing this kind of analysis, usually we are not interested in the exact
phase relationship between the signal and the sine wave, so generally only
magnitudes are shown. However, note phase angles also important important,
as they are crucial for applying the inverse Fourier transform.

### Summary

The discrete time Fourier transform works by computing the complex dot product
between the signal and a series of complex sine waves at different frequencies.

```{python}
## plot two Fourier coefficients

coefs2plot = [0,0]
coefs2plot[0] = np.argmin(np.abs(hz-4))
coefs2plot[1] = np.argmin(np.abs(hz-4.5))

# extract magnitude and angle
mag = np.abs(fCoefs[coefs2plot])
phs = np.angle(fCoefs[coefs2plot])


# show the coefficients in the complex plane
plt.plot( np.real(fCoefs[coefs2plot]) , np.imag(fCoefs[coefs2plot]) ,'o',
         linewidth=2,markersize=10,markerfacecolor='r')

# make plot look nicer
plt.plot([-2,2],[0,0],'k',linewidth=2)
plt.plot([0,0],[-2,2],'k',linewidth=2)
axislims = np.max(mag)*1.1
plt.grid()
plt.axis('square')
plt.xlim([-axislims, axislims])
plt.ylim([-axislims, axislims])
plt.xlabel('Real axis')
plt.ylabel('Imaginary axis')
plt.title('Complex plane')
plt.show()
```

## Converting indices to frequencies (1)

When we calculated the transform... How it was established the output would be
in units of Hertz? The time vector was normalized, ranging from $0$ to
$\approx 1$, which is not units of seconds. The sampling rate was not specified
was not specified within the loop either. The frequency vector was created
after the Fourier transform was complete, using the following line:

`hz = np.linspace(0, srate/2, np.floor(pnts/2) + 1)`

It is related to the Nyquist frequency, the highest frequency in a signal that
can be measured.

### Python (a)

Visualizing the complex sine waves generated within the loop will help us
to understand its properties. For each iteration, the frequency of the wave is defined by defined by the looping index minus one (`fi - 1`). For each
iteration of the loop, we plot the real and imaginary parts of wave, cosine and
sine, respectively.

```{python}
pnts     = 16 # number of time points
fourTime = np.arange(pnts)/pnts

side = np.sqrt(pnts).astype(int)
fig = plt.figure()
panels = fig.subplots(side, side, sharex=True, sharey=True)

for fi, ax in enumerate(panels.flat):

    # create complex sine wave
    csw = np.exp( -1j*2*np.pi*fi*fourTime )
    
    # and plot it
    ax.set_title(f"$f=" + str(fi) + "$")
    ax.plot(fourTime,np.real(csw))
    ax.plot(fourTime,np.imag(csw))

fig.tight_layout()
```

In the first panel, the index is $1$, and thus, the frequency and the exponent
are both is $0$ in the first iteration, so the result is $e^{0}$, a flat line
placed at height $1$. This seems counterintuitive, bit it makes complete sense:
it is a wave with a frequency of $0$ Hz, representing the average value of all
he data points in the signal (global offsets). It is called the DC component,
or DC Offset, where DC stands for Direct Current.

### Slides

Past the first pane the frequency grows higher and higher, with more cycles per unit of time. If the sampling rate remains the same, the signal eventually
reaches such a high frequency that cannot be accurately described by the amount
of points we take. This point is reached when we take just two samples per
cycle, where the samples would need to be placed on the peaks and troughs to
properly capture the fluctuations. If the wave fluctuated any faster, or if
the sampling was slower we would start to see aliasing effects [SEE CHAPTER].
~~2 points per sample is more of a theoretical limit~~.

The fastest/highest frequency that can be measured is called the Nyquist
Frequency, and corresponds to one half of the sampling rate, corresponding to
two measurements per cycle. For example, if you have measurement samples every
millisecond, that corresponds (1000 Hz), the Nyquist frequency would be
500 Hz.

### Python (b)

In our plot we reach the Nyquist frequency on the $8^{th}$ frequency index, 
half of the number of time points. Past that point we get sine waves that are
faster, and end up aliased into lower frequencies. These are called negative
frequencies [SEE VIDEO].

Each positive frequency has a corresponding negative frequency. The real part
of the positive matches the real part of the negative frequencies, whereas the
imaginary parts are shifted by $-\pi$ [SEE VIDEO].

We only interpret the frequencies between $0$ and and Nyquist, a total of
$(n/2) + 1$ points between those two frequencies (both inclusive). The $+1$ is
included to account for frequency $0$, the DC component.

## Converting indices to frequencies (2)

When it comes to implementing that theory in code, with discretization and a
finite number of numbers, it is worthwhile to slow down and explain what's
really going on with the frequencies vector.

So, the frequencies go from $0$, or DC, to Nyquist, the sampling rate divided
by two, in $(n/2)+1$ steps. However, when we work with a finite number of
numbers, things get a little bit more complicated.

Theoretically, we would need to add an adjustment factor to the higher
frequency

Imagine we have an amplitude/power spectrum, going from DC to Nyquist over
$(n/2)+1$ frequencies. However, past Nyquist there is another side of the
spectrum: the negative frequencies. There are only $n/2$ samples for these, because the positive and negative $0$ are the same.

So now let's imagine a signal that is 10 points long ($n=10$). Then we would
need $n/2$ points for each side, plus one, for the DC component: 11 frequencies for a ten point signal.

That... doesn't make sense, we should have ten frequencies for a ten point
signal.

What is actually happening is that the positive and negative frequencies
overlap at Nyquist, with both $n/2$ sharing the point, so we have to subtract
one. That gives us $(n/2)+1$ for the positive frequencies, $n/2$ for the
negative ones, and $-1$ for the overlap. The result is $10$ frequencies for a
$10$ point signal.

What does happen when we have an odd number of points? If $n=11$, we would have
$5.5$ numbers between two boundaries. That is not possible, so we round down
(floor division). Then, we have 5 points for the positive frequencies, 5 for
the negative ones, plus one for the DC component. However, positive and
negative frequencies overlap at Nyquist! so the result would be $10$
frequencies for an eleven point signal. As we need both counts to match, it is
necessary to make some changes.

What we do is space the $5$ frequencies so the last point of the positive
frequencies, and the first one in the negative ones are slightly below, and
slightly above of Nyquist. That removes the overlap, so now we have $n$
frequencies for $n$ data points: $10+1$. This remains valid because we only
reconstruct frequencies not reaching the Nyquist frequency

we add a corrective factor to get us a little bit below Nyquist, and ensure we
have $n$ frequencies for $n$ time points. What is the value of this factor?

For an even number of points we go up to the Nyquist frequency as defined by
the sampling rate.

$$
Nyq = \frac{srate}{2}
$$

For an odd number of point $Nyq^{*}$ indicates the highest frequency we can
measure in with our odd count, which includes a corrective factor

$$
Nyq^{*} = \frac{srate}{2} \frac{n-1}{n}
$$

When the number of points is very large the correction factor only makes a
small difference, whereas with a small number of points it can have a big
impact. For example, with $n=5$, the correction factor is $(5-1)/5=0.8$,
meaning the frequency only reaches $80\%$ of the Nyquist. The loss in precision
induced by the correction factor is on the order of magnitude of $1/n$. So if
you have a signal with with a million data points and you ignore this
corrective factor, your frequencies will be off by $1/10^{6}$, a very small
amount. This means many people just ignore this corrective term because in
practice this level of precision is not required, and there are other sources
of error.

## Python

In this case is not
necessary to create the signal, we calculate the maximum frequency from the
sampling rate and a number of data points. We create two frequency vectors,
both with $(n/2)+1$ entries. The first frequencies vector is hard-coded to 
reach the Nyquist frequency, whereas the second only goes up to `topfreq`. 
How `topfreq` is defined depends on whether the remainder of $n/2$ is zero
(even) or not (odd).

If it is an even number, the top frequency is the Nyquist frequency, otherwise
is Nyquist times the corrective factor $(n-1)/n$. Then we take some arbitrary
frequency index and print. 

~~The $36^{th}$ frequency, for this sampling rate and this number of time points,~~

With an even number of points ($n=100$) the element we printed is the same
for both vectors. With an odd number points (n=1201) the print is similar, for
the first significant digits, and then differ digits.

So they're both twenty nine point one, twenty nine point one, and then they
start to differ by a negligible amount for many applications using a large
number of data points.

```{python}
## code from the slides

srate = 1000
npnts = 100001

# frequencies vector
if npnts%2==0:
  topfreq = srate/2
else:
  topfreq = srate/2 * (npnts-1)/npnts

hz1 = np.linspace(0,srate/2,np.floor(npnts/2+1).astype(int))
hz2 = np.linspace(0,topfreq,np.floor(npnts/2+1).astype(int))

# some arbitary frequency to show
n = 16
print('%.9f\n%.9f'%(hz1[n],hz2[n]))
```

## A shortcut for converting indices to frequencies

Here we will learn a shortcut for converting indices to meaningful units (Hz).
Formally, frequencies should be defined by going from $0$ up to Nyquist. An
useful shortcut is to go from $0$ and the sampling rate (both inclusive) in
$n$ steps, the number of time points. This is valid for frequencies below
Nyquist, and involves a lot less typing.

The accuracy of this shortcut depends on whether the signal has an even or odd
number of time points points. To show this we rill create a sine wave with a
frequency of 15 hz, sampled at 1 kHz (1000 Hz) and calculating the power
spectrum. Then it is plotted twice, using the frequency vectors `hz1`and `hz2`, with lengths of $n+1$ and $n$, respectively.

```{python}
def plot_difference(max_time, freq=15, srate=1000, offset=.1):

    time  = np.arange(0, max_time)/srate
    npnts = len(time)

    # Notice: A simple 15-Hz sine wave!
    signal = np.sin(freq * 2 * np.pi * time)

    # its amplitude spectrum
    amplitude = 2*np.abs(scipy.fftpack.fft(signal)) / len(signal)

    # frequencies vectors
    hz1 = np.linspace(0, srate, npnts + 1)
    hz2 = np.linspace(0, srate, npnts)

    # plot it
    fig, ax = plt.subplots(1) #,figsize=(10,5)
    fmt1 = {"markerfmt": "bo", "linefmt": "b", "basefmt": "k", "label": "N+1"}
    fmt2 = {"markerfmt": "rs", "linefmt": "r", "basefmt": "k", "label": "N"}
    ax.stem(hz1[:npnts], amplitude, **fmt1)
    ax.stem(hz2,amplitude,**fmt2)
    ax.legend(loc='upper right')
    ax.set_title(str(len(time)) + ' points long')
    ax.set_ylabel('Amplitude')
    ax.set_ylim(bottom=0)
    ax.set_xlim(freq - offset, freq + offset)
    ax.set_xlabel('Frequency (Hz)')
    return fig, ax
```

When the count of time points is odd ($1001$), the frequency vector with a
length of $n$ points gives us an accurate result (15 Hz), whereas the result
with the other is lower than it should be.

```{python}
## Case 1: ODD number of data points, N is correct
plot_difference(1000 + 1);
```

With an even nuber of time points points ($1000$), the frequency vector with
$n+1$ points givie the correct result, whereas the result with the other is higher than it should be.

```{python}
## Case 2: EVEN number of data points, N+1 is correct
plot_difference(1000);
```

The frequency resolution depends on the number of time points. If we use a
signal with a length 5 seconds instead of just one ($5000$ samples), we notice
the result with $n$ points still is higher than it should be, but the distance
between them is smaller. As your signal gets longer, the less relevant it is whether length of the signal is even or odd

```{python}
## Case 3: longer signal
plot_difference(5000);
```

If you want to get your code to be really precise, then it is important to take
a moment and meticulously consider whether your signal is going to be an even length or an odd length, and then apply a correction factor [SEE PREVIOUS].
If you don't really care about that level of precision, an/or if you have a
relatively long signal, then you can actually just write the frequencies
vector you can create the frequency vector using the shortcut, and select the
valid x range (0 to Nyquist).

## Normalized time vector

Now we are going to delve deeper on on the time vector used in the Fourier
transform. For our example we will use a signal with two sine waves with
frequencies of $4$ and $6.5$ Hz, and amplitudes of $2.5$ and $1.5$,
respectively (see figure FIGURE).

```{python}
# create the signal
srate  = 1000 # hz
time   = np.arange(0, (2 * srate) + 1)/srate # time vector in seconds
# pnts   = len(time); print(pnts) # number of time points
signal = 2.5 * np.sin(2 * np.pi * 4 * time) + \
         1.5 * np.sin(2 * np.pi * 6.5 * time)
```

To apply the Fourier transform we need to prepare a normalized time vector.
It is made by creating a range of IDs going from $0$ to the number of
points minus one ($n-1$). It needs to start at $0$ so the sine wave also starts
at $0$, otherwise the angle (phase) will be shifted, even though the amplitude
(magnitude) remains the same.

The ID rage is normalized to range from $0$ to near-$1$ by dividing by the
number of time points. This means that the "time" the algorithm uses is just
the element ID depicted as a fraction of the total length. This ensures the
algorithm can work on any regularly sampled signal without any additional
information (e.g., sampling rate). Finally, note the last element of the
normalized time vector is not $1$ because the first element must be $0$ to
avoid a phase shift.

After the looping we receive the fourier coefficients, containing the amplitude
and the phase. The former is the distance from the origin in the complex plane
(magnitude), and the latter is the angle relative to the positive real axis.

```{python}
def fourier_coefs(signal, shift):

    # prepare the Fourier transform
    pnts = len(signal)
    fourTime = (np.arange(0, pnts) + shift)/pnts
    fCoefs   = np.zeros(pnts, dtype=complex)

    for fi in range(pnts):
        
        # create complex sine wave
        csw = np.exp( -1j*2*np.pi*fi*fourTime )
        
        # compute dot product between sine wave and signal
        # these are called the Fourier coefficients
        fCoefs[fi] = sum( signal * csw )/pnts

    # calculate the amplitudes
    return fCoefs
```

If we change the time vector from ranging $0 - (n-1)$ to the range $10 - (n+9)$
(same length, last element included in both cases) the result is in the
amplitude spectrum is the same, but not the phase spectrum.

```{python}
correct = fourier_coefs(signal, shift=0)
shifted = fourier_coefs(signal, shift=10)

# compute frequencies vector
hz = np.linspace(0,srate/2,int(np.floor(len(correct)/2)+1))

fig, panels = plt.subplots(2, 1, sharex=True)

# plot amplitude
panels[0].stem(hz, 2 * abs(shifted[:len(hz)]),'rs-', label='shifted')
panels[0].stem(hz, 2 * abs(correct[:len(hz)]),'ks-', label='correct')
panels[0].set_title('Amplitude spectrum')
panels[0].legend(loc='upper right')
panels[0].set_ylim([-.01,3])
panels[0].set_ylabel('Amplitude (a.u.)')

# plot angles/phase
panels[1].stem(hz, np.angle(shifted[:len(hz)]), 'rs-', label='shifted')
panels[1].stem(hz, np.angle(correct[:len(hz)]), 'ks-', label='correct')
panels[1].set_title('Phase spectrum')
panels[1].set_ylim([-np.pi,np.pi])
panels[1].set_ylabel('Phase (rad.)')
panels[1].set_xlim([0, 10])
panels[1].set_xlabel('Frequency (Hz)')

fig.align_ylabels()
```

The signals can reconstructed be reconstructed from the fourier coefficients
using the inverse fourier transform (`scipy.fftpack.ifft`). The result in the
time domain should be an exact match of the initial signal. However, if there
is a phase shift the reconstructed signal also will be displaced.

```{python}
# finally, plot reconstructed time series on top of original time series
reconTS = np.real(scipy.fftpack.ifft( shifted )) * len(signal)

plt.plot(time, signal,'k',label='Original')
plt.plot(time[::3], reconTS[::3],'r.',label='Shifted')
plt.legend()
plt.show()
```

## Positive and negative frequencies

The amplitude of a real-valued signal is split between the positive and the
negative frequencies, with each part containing half of the signal's energy.
Over the positive part frequency increases up to Nyquist ($1/2$  of the
sampling rate), then frequency decreases over the negative frequencies.

The existance of these negative frequencies is explained by Euler's formula
([and some algebra and trigonometry](https://web.archive.org/web/20130409043929/https://www.ctralie.com/PrincetonUGRAD/Tutorials/Euler/)).
:

$$
\cos k = \frac{e^{ik} + e^{-ik}}{2}
$$

The cosine is the average of two complex exponentials. Then, if $\cos (k)$
represents a real-valued wave, the expression becomes:

$$
\cos (2 \pi f t)
=
\frac{e^{i2 \pi f t} + e^{-i2 \pi f t}}{2}
$$

In order to represent the wave we need two complex exponentials, with positive
and negative powers, which represent two complex waves whose cycles go in
opposite directions (i.e., clockwise and counter-clockwise). Thus, if a signal
is real-valued, the negative frequencies just mirror the positive ones.

```{python}
is_relevant = (hz > 0) & (hz < 7.5)

positive_hz = hz[is_relevant]
positive = correct[:len(hz)][is_relevant]

negative_hz = positive_hz[::-1]
negative = correct[-len(positive):]

fig, panels = plt.subplots(2, 2, sharex="col", sharey='row')
panels[1, 1].xaxis.set_inverted(True)

# Titles
panels[0, 0].set_title("Positive frequencies")
panels[0, 1].set_title("Negative frequencies")

# plot amplitude
panels[0, 0].set_ylabel('Amplitude')
panels[0, 0].stem(positive_hz, 2 * abs(positive),'gs-', basefmt='k')
panels[0, 1].stem(negative_hz, 2 * abs(negative),'rs-', basefmt='k')

# plot phase
panels[1, 0].set_ylabel('Phase')
panels[1, 0].stem(positive_hz, np.angle(positive),'gs-', basefmt='k')
panels[1, 1].stem(negative_hz, np.angle(negative),'rs-', basefmt='k')
```

Many people just show the positive part when analyzing
real-valued signals, because the negative part just mirrors it. However, it is
important to note both positive and negative frequencies are necessary to fully
reconstruct the original signal amplitude. If only the positive frequencies are
retained, half of the amplitude of the original signal is lost
(SECTION about Fourier coefficient normalization).

See https://math.stackexchange.com/questions/525080: *"In short, negative frequencies (or indices) correspond to counter rotations. If positive frequency goes counterclockwise, then negative frequency goes clockwise."*

~~Some people (hysicists) shift the power spectrum to show negative frequencies on the left, positive frequencies on the right and zero, the DC component, in the middle.~~

# Scaling Fourier coefficients

When working with the Fourier transform, you usually want to the power spectrum
to have the same units as the signal you measured. For this, it is necessary to
apply two normalization factors. To understand this, we will run a step by step
example. Fist, we simulate a sine wave with a frequency of 4 Hz and an
amplitude of 2.5 (so we know the truth), calculate the fourier coefficents,
and plot the frequencies from $0$ to Nyquist (over $n$ steps, the data points
in the signal).

```{python}
## incorrect amplitude units without normalizations

# create the signal
srate  = 1000 # hz

def make_signal(time):
    return 2.5 * np.sin(2 * np.pi * 4 * time)


def amp_spectrum(signal, srate):

    # prepare the Fourier transform
    pnts   = len(signal)   # number of time points
    fourTime = np.array(range(pnts))/pnts
    fCoefs   = np.zeros(len(signal),dtype=complex)

    for fi in range(pnts):
        # create complex sine wave and compute dot product with signal
        csw = np.exp( -1j * 2 * np.pi * fi * fourTime )
        fCoefs[fi] = np.sum(np.multiply(signal, csw))
    
    hz = np.linspace(0, srate/2, num=int(np.floor(pnts/2.) + 1))

    return hz, np.abs(fCoefs)


def plot_spectrums(**kwargs):

    for label, (hz, amp, color) in kwargs.items():
        plt.stem(hz, amp[:len(hz)], color, basefmt="k", label=label)

    plt.legend()
    plt.xlabel('Frequency (Hz)'), plt.ylabel('Amplitude (a.u.)')
    plt.xlim(0,10)
    plt.ylim(0)
    plt.show()


short_signal = make_signal(time=np.arange(0., 1.5, 1/srate))
long_signal = make_signal(time=np.arange(0., 2.0, 1/srate))

short_hz, short_amp = amp_spectrum(short_signal, srate=srate)
long_hz, long_amp = amp_spectrum(long_signal, srate=srate)

plot_spectrums(
    long=(long_hz, long_amp, "g"),
    short=(short_hz, short_amp, "r")
    )
```

The spikes appear in the correct frequencies, but the amplitude is way larger
than than it should be. In fact, the longer the signal, larger the amplitude.
This happens because the basis of the fourier transform is the dot product
between the signal and a complex sine wave. Thus, more elements are summed up
as the signal grows longer. This can be avoided by dividing by the number
of points, normalizing for the length of the data.

```{python}
plot_spectrums(
    long=(long_hz, long_amp/len(long_amp), "g"),
    short=(short_hz, short_amp/len(short_amp), "r")
    )
```

After the first normalization the amplitude does not change with the amunt of
data points, but seems to be exactly one half of the correct value. This
happens because the amplitude gets split between the positive and the negative
frequencies. We can drop the latter, as they mirror the former, but we need to
consider we only have retained half of the energy. It is necessary to multiply
the positive amplitudes by a factor of 2. It is important to note this is not
fully accurate because the DC component ($0$ frequency) does not have a
corresponding negative frequency. So, the only the amplitudes between $0$ and
Nyquist should be doubled, but neither of these endppoints.

```{python}
plot_spectrums(
    long=(long_hz, 2 * long_amp/len(long_amp), "g"),
    short=(short_hz, 2 * short_amp/len(short_amp), "r")
    )
```

# Interpreting phase values

The phase is the angle of the vector that goes from the origin of the complex
plane to the coordinates represented by the complex Fourier coefficient.
It represents the value of the sine wave at time zero.

[Retaining] The phase relationship between the signal and the sine wave was the
primary reason why Fourier transform uses complex and not real-valued sine
waves.

On the [llustration] you can see two sound waves with equal amplitude, but
different phases. When they are transformed into the frequency domain they
have the same amplitude spectrum, but the phases are different.

```{python}
## same amplitude, different phase

# simulation parameters
srate = 1000
time  = np.arange(0.,2. + 1/srate, 1/srate)
npnts = len(time)

# generate signal
signal1 = 2.5*np.sin(2*np.pi*10*time +   0  ) # different phase values
signal2 = 2.5*np.sin(2*np.pi*10*time + np.pi/2 )


# prepare the Fourier transform
fourTime = np.array(range(npnts)) / npnts
signal1X = np.zeros((len(signal1)),dtype=complex)
signal2X = np.zeros((len(signal2)),dtype=complex)

for fi in range(npnts):
    
    # create complex sine wave
    csw = np.exp( -1j*2*np.pi*fi*fourTime )
    
    # compute dot product between sine wave and signal
    # these are called the Fourier coefficients
    signal1X[fi] = np.sum( np.multiply(signal1,csw) ) / npnts
    signal2X[fi] = np.sum( np.multiply(signal2,csw) ) / npnts


# frequencies vector
hz = np.linspace(0,srate/2,num=int(math.floor(npnts/2.)+1))

# extract correctly-normalized amplitude
signal1Amp = np.abs(signal1X[range(len(hz))])
signal1Amp[1:] = 2*signal1Amp[1:]

signal2Amp = np.abs(signal2X[range(len(hz))])
signal2Amp[1:] = 2*signal2Amp[1:]


# now extract phases
signal1phase = np.angle(signal1X[0:len(hz)])
signal2phase = np.angle(signal2X[0:len(hz)])


# plot time-domain signals
fig, panels = plt.subplots(2, 2, sharex="col", sharey="row")

#panels[0, 0].plot(time,signal1)
#panels[0, 1].plot(time,signal2,'k')

panels[0, 0].stem(hz,signal1Amp)
panels[0, 1].stem(hz,signal2Amp)

panels[1, 0].stem(hz,signal1phase)
panels[1, 0].set_xlim([0,20])

panels[1, 1].stem(hz,signal2phase)
panels[1, 1].set_xlim([0,20])

fig.tight_layout()
```

The phase spectrum is generally quite difficult to interpret visually. For
example, if we plot a trace of electrical brain activity in the time domain,
we can see the voltage fluctuations as the brain is processing a visual sensory
(appears at $t = 0$). On the the frequency there are clear features on the power spectrum, whereas it is difficult to understand what is going on the phase
spectum. That's why phase spectra rarely is plotted for real signals.

Even though phase spectrum is difficult to interpret, it defines the timing,
making it essential to reconstruct the signal.

Phase is independant from power, reason why normalization is not needed for the
phase spectrum: regardeless the length of a line, the angle respective to the
origin remains the same. However, if the length (amplitude) is zero, then the
phase is undefined, as a line with zero length cannot have an angle. In fact,
is difficult to estimate when the amplitude is too small. 

This often happens with noisy signals or when there is noise in the measurements.


Imagine we take two complex measurements, both with the same uncertainty (true
value within the same margin of tolerance). In one case the amplitude is large,
so the range of possible phase values is relatively small. On the other, the
amplitude is small enough for the origin to be included on the uncertainty
region, then the measurement could be valid for any phase!

## Averaging Fourier coefficients

The possibility of averaging Fourier coefficients has major implications for
the kinds of analyses that can be conducted with the Fourier transform. It
 also highlights the importance of phase for the results of the Fourier
 transform.

So imagine this [phasor] is a complex plane, where the two lines representing
two Fourier coefficients. Both have roughly the same distance to the origin,
So the magnitudes of these two Fourier coefficients are about the same.
However, tehir phases are quite different.

If we average the magnitudes, the result magnitude has nearly the same magnitude
as each original line. However, then we would be ignoring the phase value. If
we average the actual vectors, the result would point in a direction that's in
between these two and the length would be much smaller, because of the phase
differences between these two Fourier coefficients [see section SECTION on
the linear algebra course].

If we visualize the process, we would takie one vector and place its tail
at the head of the other. Then, the result would be, and the line between the
of the vector we moved, and the tail of the vector that remained in place.
Thus, the averaged vector can be quite shorter than the input vectors if there
is a large phase difference between the input fourier coefficients.

Averaging magnitudes and averaging vectors are both correct ways to average,
but they highlight different features of the data: the former highlights the
average amplitude of the Fourier coefficient, whereas the latter only highlights
of the amplitude that is phase-consistent between the Fourier coefficients.

### Python

For this specific example we will generate 100 time series with an identical
frequency (1) and amplitude (20 Hz), but each trial has a random phase offset
ranging 0-1 multiplied by a scale factor (e.g., $2\pi$ for a maximum of one
full cycle).

```{python}
# simulation parameters
ntrials = 100
srate   = 200 # Hz
time    = np.arange(0,1,1/srate)
pnts    = len(time)

scale_factors = [5, 10, 20]

fig = plt.figure()
panels = fig.subplot_mosaic([[sf] for sf in scale_factors], sharex=True)
panels[scale_factors[0]].set_title("Time domain")

t_data = {}
for scaler in scale_factors:

    # Make the shape using broadcasting
    t_data[scaler] = np.sin(
        (2 * np.pi * 20 * time).reshape(pnts, 1) +
        (scaler/10 * np.pi * np.random.rand(1, ntrials))
    )

    # Plot the trials on the time domain
    panels[scaler].plot(time, t_data[scaler]) # data axis is the column (mpl)
    panels[scaler].plot(time, np.mean(t_data[scaler], axis=1), color="k", lw=3)
    panels[scaler].set_ylabel(f"scaler = {scaler/10}")
    panels[scaler].set_xlim(.3, .7)

panels[scale_factors[-1]].set_xlabel('Time (sec.)')
fig.supylabel('Amplitude')
fig.tight_layout()
```

Then, we plot the individual time series, the average time series, and the
amplitude spectrum. The latter is extracted with the fast fourier transform
(`fft` function) [see section SECTION].

The result shows all the sine waves forming a mishmash crossing pattern due the
the different phase offsets. The amplitude of the average time series is much
smaller compared to the each individual trial. 

For calculating the average of magnitudes (red), we take the absolute value of
each Fourier coefficient, scale them up by two and take the average. The
average of fouriere coeffiints (average of vectors, black) is calculated by
averaging first and then extracting the magnitude. Even though the code looks
nearly the same, the average of magnitudes (red) has a an amplitude of one at
20 Hz, whereas the average of vectors (black) is very small when the scale
factor is $2\pi$. However, it grows larger if we decrease the phase jitter
(amount of randomization), whith the amplitude at 20 Hz being nearly the same
when the scale factor is $0.1$. On the time dmain we can observe how the waves
are more aligned the the smaller is the scale factor.


```{python}
for color, (scaler, t_trials) in zip(["m", "g", "c"], t_data.items()):

    freq_trials = scipy.fftpack.fft(t_trials,axis=0) / pnts
    hz = np.linspace(0, srate/2, int(np.floor(pnts/2)))

    # averaging option 2: magnitude, then complex Fourier coefficients
    mag_avg = np.mean( 2*np.abs(freq_trials),axis=1 )[0:len(hz)]
    
    # averaging option 1: complex Fourier coefficients, then magnitude
    vector_avg = 2*np.abs( np.mean(freq_trials,axis=1) )[0:len(hz)]

    # plot the average magnitude (only the first time)
    if color == "m":
        plt.stem(hz, mag_avg, f'ks-', label='Magnitude average')
    
    # Plot the average vector
    vector_label = f"Coef average (scale = {scaler/10})"
    plt.stem(hz, vector_avg, f'{color}s-', label=vector_label)

plt.xlabel('Frequency (Hz)')
plt.ylabel('Amplitude')
plt.title('Frequency domain')
plt.legend()
```

## VIDEO: The DC coefficient

The DC component reflects the amplitude of a signal with zero frequency. It
reflects the average of the entire signal because the sine wave used for the
dot product only contains ones. Thus, when we divide by the number of points
in the signal, we just get the mean.

Note the DC component does not affect the sinusoidal components, as it captured
by the mean offset, the zero frequency component [integrate better].


$$
\frac{1}{4}
%\times
\begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix}
%\times
\begin{bmatrix} 3 \\ 3 \\ 2 \\ 2 \end{bmatrix}
=
\frac{1 \times 3 + 1 \times 3 + 1 \times 2 + 1 \times 2}{4}
=
2.5
$$

THe division by the point count is one of the two normalizations necessary to
transform the Fourier coefficents into the units of the original signal.

### Python

First we generate a signal with a global offset of $1.5$. Then we compute the
Fourier transform, and apply the normalization factors, dividing by the number
of points to account for the signal length, and multiply by two to account for
the removal of the of the negative frequency.

The amplitude spectrum shows just two two frequencies with non-zero energies,
corresponding to 0 and 4 Hz. The amplitude of the latter (2.5 Hz) matches our
simulation, but the former is 3 Hz, twice as large than it should be (1.5 Hz).
This is telated to the positive and negative frequencies. When we transform a
real-valued signal to the the frequency domain, its energy is split into
these twwo frequencies. However, the zero frequency does not have a negative
complement. Thus, the only the frequencies past zero Hertz but smaller than
Nyquist should be multiplied by 2.

```{python}
## incorrect DC reconstruction without careful normalization

# create the signal
srate  = 1000 # hz
time   = np.arange(0.,2.,1/srate) # time vector in seconds
pnts   = len(time) # number of time points
signal =  1.5 + 2.5*np.sin( 2*np.pi*4*time )


# prepare the Fourier transform
fourTime = np.array(range(pnts))/pnts
fCoefs   = np.zeros(len(signal),dtype=complex)

for fi in range(pnts):
    # create complex sine wave and compute dot product with signal
    csw = np.exp( -1j*2*np.pi*fi*fourTime )
    fCoefs[fi] = np.sum( np.multiply(signal,csw) )


# extract amplitudes, scale theby number of points and negati freq. removal
ampls = np.abs(fCoefs/pnts);
ampls[1:] *= 2

# compute frequencies vector
hz = np.linspace(0,srate/2,num=int(math.floor(pnts/2.)+1))

plt.stem(hz,ampls[0:len(hz)])
plt.xlim(-.1,10)
plt.xlabel('Frequency (Hz)'), plt.ylabel('Amplitude (a.u.)')
```

## Amplitude spectrum vs. power spectrum

The power and the amplitude spectrum are related to each other, with the latter
being the amplitude squared power (or, alternatively, amplitude is the
principal square root of power).

If we plot a fourier coefficient on the complex plane, the amplitude would be
the magnitude, the distance away from the origin. Squaring amplitudes under
one causes them to become even smaller, whereas amplitudes larger than one
become even larger. This enhances the differences across the spectrum, offering
a "cleaner" result.

### Python


```{python}
# simulation parameters
srate = 1000
time  = np.arange(0,.85,1/srate)
npnts = len(time)

# generate signal
signal = 2.5*np.sin(2*np.pi*10*time)

# Fourier transform and frequencies
signalX = scipy.fftpack.fft(signal) / npnts
hz = np.linspace(0,srate/2,int(np.floor(len(time)/2)+1))


# extract correctly-normalized amplitude
signalAmp = np.abs(signalX[0:len(hz)])
signalAmp[1:] = 2*signalAmp[1:]

# and power
signalPow = signalAmp**2


plt.figure(figsize=(12,3))

# plot time-domain signal
plt.subplot2grid((1,3),(0,0))
plt.plot(time,signal)
plt.xlabel('Time (ms)')
plt.ylabel('Amplitude')
plt.title('Time domain')


# plot frequency domain spectra
plt.subplot2grid((1,3),(0,1))
plt.plot(hz,signalAmp,'ks-',label='Amplitude')
plt.plot(hz,signalPow,'rs-',label='Power')

plt.xlim([0,20])
plt.legend()
plt.xlabel('Frequency (Hz)')
plt.ylabel('Amplitude or power')
plt.title('Frequency domain')

# plot dB power
plt.subplot2grid((1,3),(0,2))
plt.plot(hz,10*np.log10(signalPow),'ks-')
plt.xlim([0,20])
plt.xlabel('Frequency (Hz)')
plt.ylabel('Decibel power')

plt.tight_layout()

plt.show()
```

We just generate a sine wave with a frequency of 10 hertz, an amplitude of 2.5,
and a length of 850 milliseconds. Note this means the number of cycles
appearing on the signal will not be an integer, and the Fourier transform
will ¿allocate? energy to other frequencies to be able to represent our signal
(see section on aliasing and non-stationarities). It allows to exeemplify the
differences between the amplitude (black) and the power spectrum (red): the
latter shows a higher peak, and the energy of the surrounding frequencies is
lower because these are under 1 (within the unit circle here).

The power and the amplitude spectrum are equally accurate representations of
the signal, but the amplitude spectrum highlights the subtleties of the signal,
whereas a bit better, whereas the power spectrum highlights the most prominent
features.

In many disciplines other disciplines, it's common to apply some sort of
transformation or normalization of the power spectrum, sucha s the decibel
(e.g., 10 times the log 10 of the raw power.).

### Parseval's Theorem.

Parceval's theorem states that the sum of all the power values squared equals
the sum of all the time points squared: the total amount of energy in the time
series is the same as the total amount of energy in the power spectrum.

$$
\Sigma^n_{t=1}
\left| x_t \right|^{2}
=
\frac{1}{n} \left| X_t \right|^{2}
$$

Here we use focus on the amplitude spectrum because it makes trivial to seel if
the signal on the the frequency domain frequency domain matches matches the
one we simulated on the time domain. If we were using the power spectrum we
would need some math to check wether the calculations are correct. Nonetheles,
using the amplitude or the power spectum, or a transformation of thereof
depends on the specific discipline.

### A note about terminology of Fourier features

To be added